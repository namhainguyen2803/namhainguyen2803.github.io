<!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="google-site-verification" content="" />
    <meta name="msvalidate.01" content="" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hai Nguyen</title>
    <meta name="author" content="Hai Nguyen" />
    <meta name="description" content="My personal website. " />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />
    <link rel="shortcut icon" href="/assets/img//assets/img/favicon.ico" />
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://namhainguyen2803.github.io/">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>
  <body class="fixed-top-nav ">
    <header>
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <div class="navbar-brand social">
            <a href="mailto:%6B%68%61%69%6E%62@%75%74%65%78%61%73.%65%64%75" title="email">
              <i class="fas fa-envelope"></i>
            </a>
            <a href="https://scholar.google.com/citations?user=im5fNaQAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer">
              <i class="ai ai-google-scholar"></i>
            </a>
            <a href="https://www.researchgate.net/profile/Khai-Nguyen-37/" title="ResearchGate" target="_blank" rel="noopener noreferrer">
              <i class="ai ai-researchgate"></i>
            </a>
            <a href="https://github.com/namhainguyen2803" title="GitHub" target="_blank" rel="noopener noreferrer">
              <i class="fab fa-github"></i>
            </a>
            <a href="https://www.linkedin.com/in/khai-nguyen-307895155" title="LinkedIn" target="_blank" rel="noopener noreferrer">
              <i class="fab fa-linkedin"></i>
            </a>
            <a href="https://twitter.com/khainb_ml" title="Twitter" target="_blank" rel="noopener noreferrer">
              <i class="fab fa-twitter"></i>
            </a>
          </div>
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>
          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">
              <li class="nav-item active">
                <a class="nav-link" href="/">About <span class="sr-only">(current)</span>
                </a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>
    <div class="container mt-5">
      <div class="post">
        <header class="post-header">
          <h1 class="post-title"> Hai Nguyen </h1>
          <p class="desc">Ph.D. Candidate at <a href="https://stat.utexas.edu/" target="_blank" rel="noopener noreferrer">Department of Statistics and Data Sciences</a>, <a href="https://www.utexas.edu/" target="_blank" rel="noopener noreferrer">University of Texas at Austin</a>
          </p>
        </header>
        <article>
          <div class="profile float-right">
            <figure>
              <picture>
                <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/khai2-480.webp">
                </source>
                <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/khai2-800.webp">
                </source>
                <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/khai2-1400.webp">
                </source>
                <img src="/assets/img/khai2.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="khai2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
              </picture>
            </figure>
            <div class="address"></div>
          </div>
          <div class="clearfix">
            <p>Hi! I’m Hai, a third-year student at <a href="https://stat.utexas.edu/" target="_blank" rel="noopener noreferrer">Department of Statistics and Data Sciences</a>, <a href="https://www.utexas.edu/" target="_blank" rel="noopener noreferrer">University of Texas at Austin</a>. I am fortunate to be advised by Professor <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a> and Professor <a href="https://www.ma.utexas.edu/component/cobalt/item/15-mathematics/364-mueller-peter?Itemid=1259" target="_blank" rel="noopener noreferrer">Peter Müller</a>. I am associated with Institute for Foundations of Machine Learning ( <a href="https://www.ifml.institute/" target="_blank" rel="noopener noreferrer">IFML</a>) and I am a visiting student at <a href="https://odin.mdacc.tmc.edu/~wwang7/people.html" target="_blank" rel="noopener noreferrer">Statistical Information Lab</a> at <a href="https://www.mdanderson.org/" target="_blank" rel="noopener noreferrer">The University of Texas MD Anderson Cancer Center</a>. I graduated from <a href="https://soict.hust.edu.vn/" target="_blank" rel="noopener noreferrer">Hanoi University of Science and Technology</a> with a Computer Science Bachelor’s degree. Before joining UT Austin, I was an AI Research Resident at <a href="http://www.vinai.io" target="_blank" rel="noopener noreferrer">VinAI Research</a> under the supervision of <a href="https://sites.google.com/site/buihhung/" target="_blank" rel="noopener noreferrer">Dr. Hung Bui</a>. </p>
            <p>
              <strong>Research:</strong> My research focuses on both fundamental problems and applied problems in probabilistic machine learning, deep learning, and statistics.
            </p>
            <p>
              <strong>
                <em>1. Computational Optimal Transport.</em>
              </strong> My research makes <a href="https://en.wikipedia.org/wiki/Transportation_theory_(mathematics)" target="_blank" rel="noopener noreferrer">Optimal Transport</a> scalable in statistical inference (low time complexity, low space complexity, low sample complexity) via the one-dimensional projection approach which is known as sliced optimal transport (sliced Wasserstein distance). My work focuses on three key aspects of sliced Wasserstein: numerical approximation, projecting operator, and slicing distribution.
            </p>
            <p>
              <strong>
                <em>2. Efficiency, Scalability, Interpretability, and Trustworthiness of AI.</em>
              </strong> My research enhances the performance of 3D vision models, speeds up the training of generative models, adapts prediction models to new unseen domains, explains multimodal transferable representation, and ensures fairness and robustness in learning processes.
            </p>
          </div>
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive" style="max-height: 10vw">
              <table class="table table-sm table-borderless">
                <tr>
                  <th scope="row">May 1, 2024</th>
                  <td> 1 paper <strong>Sliced Wasserstein with Random-Path Projecting Directions</strong> is accepted at <strong>ICML2024</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Feb 27, 2024</th>
                  <td> 1 paper <strong>Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning</strong> is accepted at <strong>CVPR2024</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Jan 19, 2024</th>
                  <td> 2 papers <strong>Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts</strong>, <strong>On Parameter Estimation in Deviated Gaussian Mixture of Experts</strong> are accepted at <strong>AISTATS2024</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Jan 16, 2024</th>
                  <td> 4 papers <strong>Quasi-Monte Carlo for 3D Sliced Wasserstein - Spotlight Presentation</strong>, <strong>Sliced Wasserstein Estimation with Control Variates</strong>, <strong>Diffeomorphic Deformation via Sliced Wasserstein Distance Optimization for Cortical Surface Reconstruction</strong>, and <strong>Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation</strong> are accepted at <strong>ICLR2024</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Sep 21, 2023</th>
                  <td> 4 papers <strong>Energy-Based Sliced Wasserstein Distance</strong>, <strong>Markovian sliced Wasserstein distances: Beyond independent projections</strong>, <strong>Designing robust Transformers using robust kernel density estimation</strong>, and <strong>Minimax optimal rate for parameter estimation in multivariate deviated models</strong> are accepted at <strong>NeurIPS2023</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Apr 24, 2023</th>
                  <td> 1 paper <strong>Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</strong> is accepted at <strong>ICML2023</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Jan 20, 2023</th>
                  <td> 1 paper <strong>Hierarchical Sliced Wasserstein Distance</strong> is accepted at <strong>ICLR 2023</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Sep 14, 2022</th>
                  <td> 4 papers <strong>Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution</strong>, <strong>Amortized Projection Optimization for Sliced Wasserstein Generative Models</strong>, <strong>Improving Transformer with an Admixture of Attention Heads</strong> , and <strong>FourierFormer: Transformer Meets Generalized Fourier Integral Theorem</strong> are accepted at <strong>NeurIPS 2022</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Apr 24, 2022</th>
                  <td> 2 papers <strong>Improving Mini-batch Optimal Transport via Partial Transportation</strong> and <strong>On Transportation of Mini-batches: A Hierarchical Approach</strong> are accepted at <strong>ICML 2022</strong>. </td>
                </tr>
                <tr>
                  <th scope="row">Jan 24, 2021</th>
                  <td> 2 papers <strong>Distributional Sliced-Wasserstein and Applications to Generative Modeling - Spotlight Presentation</strong> and <strong>DImproving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein</strong> are accepted at <strong>ICLR2021</strong>. </td>
                </tr>
              </table>
            </div>
          </div>
          <div class="publications">
            <h2>Selected Publications <a href="https://namhainguyen2803.github.io/publications/">[Full List] </a>
            </h2> (*) denotes equal contribution <ol class="bibliography">
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#4E6C50">
                      <a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML</a>
                    </abbr>
                  </div>
                  <div id="nguyen2024sliced" class="col-sm-9">
                    <div class="title">Sliced Wasserstein with Random-Path Projecting Directions</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, Shujian Zhang,  Tam Le and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Nhat Ho' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, 15); ">1 more author</span>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Machine Learning</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2401.15889.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/namhainguyen2803/RPSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced Wasserstein distance in applications. Previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. In this work, we propose an optimization-free slicing distribution that provides a fast sampling for the Monte Carlo estimation of expectation. In particular, we introduce the random-path projecting direction (RPD) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. From the RPD, we derive the random-path slicing distribution (RPSD) and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW). We then discuss the topological, statistical, and computational properties of RPSW and IWRPSW. Finally, we showcase the favorable performance of RPSW and IWRPSW in gradient flow and the training of denoising diffusion generative models on images.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#820000">
                      <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a>
                    </abbr>
                    <span class="award badge">Spotlight</span>
                  </div>
                  <div id="nguyen2023quasi" class="col-sm-9">
                    <div class="title">Quasi-Monte Carlo for 3D Sliced Wasserstein</div>
                    <div class="author">
                      <em>Khai Nguyen</em>,  <a href="https://nbariletto.github.io/" target="_blank" rel="noopener noreferrer">Nicolas Bariletto</a>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Learning Representations</em>
                    </div>
                    <span class="honor"> Spotlight Presentation [Top 5%] </span>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2309.11713.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/namhainguyen2803/Quasi-SW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-discrepancy sequences. For theoretical properties, we prove the asymptotic convergence of QSW and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed QSW and RQSW variants.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#820000">
                      <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a>
                    </abbr>
                  </div>
                  <div id="nguyen2023control" class="col-sm-9">
                    <div class="title">Sliced Wasserstein Estimation with Control Variates</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Learning Representations</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2305.00402.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/namhainguyen2803/CV-SW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>The sliced Wasserstein (SW) distances between two probability measures are defined as the expectation of the Wasserstein distance between two one-dimensional projections of the two measures. The randomness comes from a projecting direction that is used to project the two input measures to one dimension. Due to the intractability of the expectation, Monte Carlo integration is performed to estimate the value of the SW distance. Despite having various variants, there has been no prior work that improves the Monte Carlo estimation scheme for the SW distance in terms of controlling its variance. To bridge the literature on variance reduction and the literature on the SW distance, we propose computationally efficient control variates to reduce the variance of the empirical estimation of the SW distance. The key idea is to first find Gaussian approximations of projected one-dimensional measures, then we utilize the closed-form of the Wasserstein-2 distance between two Gaussian distributions to design the control variates. In particular, we propose using a lower bound and an upper bound of the Wasserstein-2 distance between two fitted Gaussians as two computationally efficient control variates. We empirically show that the proposed control variate estimators can help to reduce the variance considerably when comparing measures over images and point-clouds. Finally, we demonstrate the favorable performance of the proposed control variate estimators in gradient flows to interpolate between two point-clouds and in deep generative modeling on standard image datasets, such as CIFAR10 and CelebA.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#0048BA">
                      <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a>
                    </abbr>
                  </div>
                  <div id="nguyen2023energy" class="col-sm-9">
                    <div class="title">Energy-Based Sliced Wasserstein Distance</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>Neural Information Processing Systems</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2304.13586.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/namhainguyen2803/EBSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter-free and has the density proportional to an energy function of the projected one-dimensional Wasserstein distance. We then derive a novel sliced Wasserstein metric, energy-based sliced Waserstein (EBSW) distance, and investigate its topological, statistical, and computational properties via importance sampling, sampling importance resampling, and Markov Chain methods. Finally, we conduct experiments on point-cloud gradient flow, color transfer, and point-cloud reconstruction to show the favorable performance of the EBSW.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#0048BA">
                      <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a>
                    </abbr>
                  </div>
                  <div id="nguyen2023markov" class="col-sm-9">
                    <div class="title">Markovian Sliced Wasserstein Distances: Beyond Independent Projections</div>
                    <div class="author">
                      <em>Khai Nguyen</em>,  <a href="https://www.cs.utexas.edu/~tzren/" target="_blank" rel="noopener noreferrer">Tongzheng Ren</a>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>Neural Information Processing Systems</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2301.03749.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/UT-Austin-Data-Science-Group/MSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Sliced Wasserstein (SW) distance suffers from redundant projections due to independent uniform random projecting directions. To partially overcome the issue, max K sliced Wasserstein (Max-K-SW) distance (K≥1), seeks the best discriminative orthogonal projecting directions. Despite being able to reduce the number of projections, the metricity of Max-K-SW cannot be guaranteed in practice due to the non-optimality of the optimization. Moreover, the orthogonality constraint is also computationally expensive and might not be effective. To address the problem, we introduce a new family of SW distances, named Markovian sliced Wasserstein (MSW) distance, which imposes a first-order Markov structure on projecting directions. We discuss various members of MSW by specifying the Markov structure including the prior distribution, the transition distribution, and the burning and thinning technique. Moreover, we investigate the theoretical properties of MSW including topological properties (metricity, weak convergence, and connection to other distances), statistical properties (sample complexity, and Monte Carlo estimation error), and computational properties (computational complexity and memory complexity). Finally, we compare MSW distances with previous SW variants in various applications such as gradient flows, color transfer, and deep generative modeling to demonstrate the favorable performance of MSW.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#4E6C50">
                      <a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML</a>
                    </abbr>
                  </div>
                  <div id="nguyen2023selfattention" class="col-sm-9">
                    <div class="title">Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</div>
                    <div class="author">
                      <em>Khai Nguyen</em>*, Dang Nguyen*, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Machine Learning</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2301.04791.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/hsgser/Self-Amortized-DSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Max sliced Wasserstein (Max-SW) distance has been widely known as a solution for redundant projections of sliced Wasserstein (SW) distance. In applications that have various independent pairs of probability measures, amortized projection optimization is utilized to predict the “max" projecting directions given two input measures instead of using projected gradient ascent multiple times. Despite being efficient, the first issue of the current framework is the violation of permutation invariance property and symmetry property. To address the issue, we propose to design amortized models based on self-attention architecture. Moreover, we adopt efficient self-attention architectures to make the computation linear in the number of supports. Secondly, Max-SW and its amortized version cannot guarantee metricity property due to the sub-optimality of the projected gradient ascent and the amortization gap. Therefore, we propose to replace Max-SW with distributional sliced Wasserstein distance with von Mises-Fisher (vMF) projecting distribution (v-DSW). Since v-DSW is a metric with any non-degenerate vMF distribution, its amortized version can guarantee the metricity when predicting the best discriminate projecting distribution. With the two improvements, we derive self-attention amortized distributional projection optimization and show its appealing performance in point-cloud reconstruction and its downstream applications.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#820000">
                      <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a>
                    </abbr>
                  </div>
                  <div id="nguyen2022hsw" class="col-sm-9">
                    <div class="title">Hierarchical Sliced Wasserstein Distance</div>
                    <div class="author">
                      <em>Khai Nguyen</em>,  <a href="https://www.cs.utexas.edu/~tzren/" target="_blank" rel="noopener noreferrer">Tongzheng Ren</a>,  <a href="https://huynm99.github.io/" target="_blank" rel="noopener noreferrer">Huy Nguyen</a> and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Litu Rout, Tan Nguyen, Nhat Ho' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, 15); ">3 more authors</span>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Learning Representations</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2209.13570.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/UT-Austin-Data-Science-Group/HSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#0048BA">
                      <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a>
                    </abbr>
                  </div>
                  <div id="nguyen2022revisiting" class="col-sm-9">
                    <div class="title">Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>Neural Information Processing Systems</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2204.01188.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/UT-Austin-Data-Science-Group/CSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#0048BA">
                      <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a>
                    </abbr>
                  </div>
                  <div id="nguyen2022amortized" class="col-sm-9">
                    <div class="title">Amortized Projection Optimization for Sliced Wasserstein Generative Models</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>Neural Information Processing Systems</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2203.13417.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/UT-Austin-Data-Science-Group/AmortizedSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#4E6C50">
                      <a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML</a>
                    </abbr>
                  </div>
                  <div id="pmlr-v162-nguyen22e" class="col-sm-9">
                    <div class="title">Improving Mini-batch Optimal Transport via Partial Transportation</div>
                    <div class="author">
                      <em>Khai Nguyen</em>*, Dang Nguyen*,  The-Anh Vu Le and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tung Pham, Nhat Ho' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, 15); ">2 more authors</span>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Machine Learning</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mappings, namely, mappings that are optimal on the mini-batch level but are partially wrong in the comparison with the optimal transportation plan between the original measures. Motivated by the misspecified mappings issue, we propose a novel mini-batch method by using partial optimal transport (POT) between mini-batch empirical measures, which we refer to as mini-batch partial optimal transport (m-POT). Leveraging the insight from the partial transportation, we explain the source of misspecified mappings from the m-OT and motivate why limiting the amount of transported masses among mini-batches via POT can alleviate the incorrect mappings. Finally, we carry out extensive experiments on various applications such as deep domain adaptation, partial domain adaptation, deep generative model, color transfer, and gradient flow to demonstrate the favorable performance of m-POT compared to current mini-batch methods.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#4E6C50">
                      <a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML</a>
                    </abbr>
                  </div>
                  <div id="pmlr-v162-nguyen22d" class="col-sm-9">
                    <div class="title">On Transportation of Mini-batches: A Hierarchical Approach</div>
                    <div class="author">
                      <em>Khai Nguyen</em>,  <a href="https://hsgser.github.io/" target="_blank" rel="noopener noreferrer">Dang Nguyen</a>,  Quoc Nguyen and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Tung Pham, Hung Bui, Dinh Phung, Trung Le, Nhat Ho' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, 15); ">5 more authors</span>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Machine Learning</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Mini-batch optimal transport (m-OT) has been successfully used in practical applications that involve probability measures with a very high number of supports. The m-OT solves several smaller optimal transport problems and then returns the average of their costs and transportation plans. Despite its scalability advantage, the m-OT does not consider the relationship between mini-batches which leads to undesirable estimation. Moreover, the m-OT does not approximate a proper metric between probability measures since the identity property is not satisfied. To address these problems, we propose a novel mini-batch scheme for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds the optimal coupling between mini-batches and it can be seen as an approximation to a well-defined distance on the space of probability measures. Furthermore, we show that the m-OT is a limit of the entropic regularized version of the BoMb-OT when the regularized parameter goes to infinity. Finally, we carry out experiments on various applications including deep generative models, deep domain adaptation, approximate Bayesian computation, color transfer, and gradient flow to show that the BoMb-OT can be widely applied and performs well in various applications.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#820000">
                      <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a>
                    </abbr>
                  </div>
                  <div id="nguyen2021improving" class="col-sm-9">
                    <div class="title">Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, Son Nguyen,  <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a> and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tung Pham, Hung Bui' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, 15); ">2 more authors</span>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Learning Representations</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2010.01787.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the prior of latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time of the vMF distribution in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge" style="background-color:#820000">
                      <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a>
                    </abbr>
                    <span class="award badge">Spotlight</span>
                  </div>
                  <div id="nguyen2021distributional" class="col-sm-9">
                    <div class="title">Distributional Sliced-Wasserstein and Applications to Generative Modeling</div>
                    <div class="author">
                      <em>Khai Nguyen</em>,  <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>,  Tung Pham and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hung Bui' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, 15); ">1 more author</span>
                    </div>
                    <div class="periodical">
                      <em>International Conference on Learning Representations</em>
                    </div>
                    <span class="honor"> Spotlight Presentation [Top 3.78%] </span>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2002.07367.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                      <a href="https://github.com/VinAIResearch/DSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Sliced-Wasserstein distance (SW) and its variant, Max Sliced-Wasserstein distance (Max-SW), have been used widely in the recent years due to their fast computation and scalability even when the probability measures lie in a very high dimensional space. However, SW requires many unnecessary projection samples to approximate its value while Max-SW only uses the most important projection, which ignores the information of other useful directions. In order to account for these weaknesses, we propose a novel distance, named Distributional Sliced-Wasserstein distance (DSW), that finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections themselves. We show that the DSW is a generalization of Max-SW, and it can be computed efficiently by searching for the optimal push-forward measure over a set of probability measures over the unit sphere satisfying certain regularizing constraints that favor distinct directions. Finally, we conduct extensive experiments with large-scale datasets to demonstrate the favorable performances of the proposed distances over the previous sliced-based distances in generative modeling applications.</p>
                    </div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
          <div class="publications">
            <h2>Selected Preprints <a href="https://namhainguyen2803.github.io/publications/">[Full List]</a>
            </h2> (*) denotes equal contribution <ol class="bibliography">
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge">Preprint</abbr>
                  </div>
                  <div id="nguyen2024hybrid" class="col-sm-9">
                    <div class="title">Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>Under Review</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2404.15378.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    </div>
                    <div class="abstract hidden">
                      <p>Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability. However, the SW and the GSW are only defined between distributions supported on a homogeneous domain. This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains. Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set. To address the issue, we propose two new slicing operators i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT). In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments. By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions. We then discuss the topological, statistical, and computational properties of H2SW. Finally, we demonstrate the favorable performance of H2SW in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison.</p>
                    </div>
                  </div>
                </div>
              </li>
              <li>
                <div class="row">
                  <div class="col-sm-2 abbr">
                    <abbr class="badge">Preprint</abbr>
                  </div>
                  <div id="nguyen2024fair" class="col-sm-9">
                    <div class="title">Marginal Fairness Sliced Wasserstein Barycenter</div>
                    <div class="author">
                      <em>Khai Nguyen</em>, Hai Nguyen, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
                    </div>
                    <div class="periodical">
                      <em>Under Review</em>
                    </div>
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                      <a href="https://arxiv.org/pdf/2405.07482.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    </div>
                    <div class="abstract hidden">
                      <p>The sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.</p>
                    </div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
          <div class="social">
            <div class="contact-icons">
              <a href="mailto:%6B%68%61%69%6E%62@%75%74%65%78%61%73.%65%64%75" title="email">
                <i class="fas fa-envelope"></i>
              </a>
              <a href="https://scholar.google.com/citations?user=im5fNaQAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer">
                <i class="ai ai-google-scholar"></i>
              </a>
              <a href="https://www.researchgate.net/profile/Khai-Nguyen-37/" title="ResearchGate" target="_blank" rel="noopener noreferrer">
                <i class="ai ai-researchgate"></i>
              </a>
              <a href="https://github.com/namhainguyen2803" title="GitHub" target="_blank" rel="noopener noreferrer">
                <i class="fab fa-github"></i>
              </a>
              <a href="https://www.linkedin.com/in/khai-nguyen-307895155" title="LinkedIn" target="_blank" rel="noopener noreferrer">
                <i class="fab fa-linkedin"></i>
              </a>
              <a href="https://twitter.com/khainb_ml" title="Twitter" target="_blank" rel="noopener noreferrer">
                <i class="fab fa-twitter"></i>
              </a>
            </div>
            <div class="contact-note"></div>
          </div>
        </article>
      </div>
    </div>
    <footer class="fixed-bottom">
      <div class="container mt-0"> © Copyright 2024 Khai Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
    <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    <script type="text/javascript">
      $(function() {
        $('[data-toggle="tooltip"]').tooltip()
      });
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
    <script defer src="/assets/js/zoom.js"></script>
    <script defer src="/assets/js/common.js"></script>
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: "ams"
        }
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKXM8P5V7J"></script>
    <script>
      function gtag() {
        window.dataLayer.push(arguments)
      }
      window.dataLayer = window.dataLayer || [], gtag("js", new Date), gtag("config", "G-ZKXM8P5V7J");
    </script>
    <script async src="https://cdn.panelbear.com/analytics.js?site="></script>
    <script>
      window.panelbear = window.panelbear || function() {
        (window.panelbear.q = window.panelbear.q || []).push(arguments)
      }, panelbear("config", {
        site: ""
      });
    </script>
  </body>
</html>